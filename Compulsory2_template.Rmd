---
title: "Compulsory Exercise 2: Title (give your project an informative title)"
author:
- Adam DJAROUD \#1.
- Full name for group member \#2.
- Full name for group member \#3.
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: 
  - \usepackage{amsmath}
  - \geometry{a4paper, margin=0.7in}
  - \usepackage{titlesec}
  - \titlespacing{\section}{0pt}{1pt}{1pt}
  - \titlespacing{\subsection}{0pt}{1pt}{1pt} 
output:
  # html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library(knitr)
# Feel free to change the setting as you see fit
knitr::opts_chunk$set(echo = TRUE,
                      tidy = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      strip.white = TRUE,
                      prompt = FALSE,
                      cache = TRUE,
                      size = "scriptsize",
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = "center")

```

```{r, eval=TRUE, echo=FALSE}
library("knitr")
library("rmarkdown")
```

# Abstract


# Introduction


# Descriptive data analysis/statistics

First, we will pre process the data by standardizing the numeric columns, the dataset was clean, without missing values.

```{r, echo=FALSE}
library(readr)
data <- read.csv("./heart.csv")
#head(data)
#summary(data)
set.seed(123)
```


```{r}
# standardization of numeric columns
numeric_columns <- sapply(data, is.numeric)
numeric_columns["HeartDisease"] <- FALSE
print(numeric_columns)
data[numeric_columns] <- scale(data[numeric_columns])
#summary(data)
```

```{r}
data[] <- lapply(data, function(x) {
  if (is.character(x) || is.factor(x)) {
    as.numeric(factor(x)) - 1  
  } else {
    x
  }
})
str(data)
```

In this dataset, there are 12 columns, so 12 parameters and 918 rows, so 918 observations. We have quantitative parameters, which are : Age, RestingBP, Cholesterol, MaxHR and OldPeak. The qualitative parameters are Sex, ChestPainType, FastingBS, RestingECG, ExerciseAngina, ST_Slope and HeartDisease.

Here are histograms for the qualitative variables.
```{r, fig.width=9, fig.height=6, echo=FALSE}
library(ggplot2)
library(gridExtra)

plot1 <- ggplot(data, aes(x = factor(Sex))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Heart disease", x = "Heart disease", y = "Number") +
  theme_minimal()

plot2 <- ggplot(data, aes(x = ChestPainType, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "Sex depending HD",
       x = "Sex",
       fill = "HD") +
  theme_minimal()

plot3 <- ggplot(data, aes(x = Sex, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "Type of thoracic pain depending HD",
       x = "Type of thoracic pain",
       fill = "HD") +
  theme_minimal()

plot4 <- ggplot(data, aes(x = FastingBS, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "FastingBS depending HD",
       x = "FastingBS",
       fill = "HD") +
  theme_minimal()

plot5 <- ggplot(data, aes(x = RestingECG, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "RestingECG depending HD",
       x = "RestingECG",
       fill = "HD") +
  theme_minimal()

plot6 <- ggplot(data, aes(x = ExerciseAngina, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "ExerciseAngina depending HD",
       x = "ExerciseAngina",
       fill = "HD") +
  theme_minimal()

plot7 <- ggplot(data, aes(x = ST_Slope, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "ST_Slope depending HD",
       x = "ST_Slope",
       fill = "HD") +
  theme_minimal()

library(patchwork)
reduce_text_size <- function(p, size = 8) {
  p + theme(title = element_text(size = size),
            axis.title.x = element_text(size = size - 1),
            axis.title.y = element_text(size = size - 1),
            axis.text.x = element_text(size = size - 2),
            axis.text.y = element_text(size = size - 2),
            legend.text = element_text(size = size - 2),
            legend.title = element_text(size = size - 2))
}

plot1_small <- reduce_text_size(plot1)
plot2_small <- reduce_text_size(plot2)
plot3_small <- reduce_text_size(plot3)
plot4_small <- reduce_text_size(plot4)
plot5_small <- reduce_text_size(plot5)
plot6_small <- reduce_text_size(plot6)
plot7_small <- reduce_text_size(plot7)


plot_combined_three_col <- plot1_small + plot2_small + plot3_small +
  plot4_small + plot5_small + plot6_small + plot7_small +
  plot_layout(ncol = 4, widths = c(1, 1, 1))

plot_combined_three_col
```


And here are the distribution and the correlation between quantitative variables.
```{r, echo=FALSE}
library(GGally)
library(ggplot2)
```
```{r}
pm <- ggpairs(data,
              columns = c("Age", "RestingBP", "Cholesterol", "MaxHR", "Oldpeak"), 
              mapping = aes(color = factor(HeartDisease)))
print(pm)
```

## KNN

In this part, we will use the KNN method. KNN is a non-parametric method where a data point is classified based on the majority vote of its K nearest neighbors in the feature space. The choice of K is crucial for performance. This is why the first thing is to find the best K

```{r}
data$HeartDisease <- as.factor(data$HeartDisease)
split <- sample(1:nrow(data), size = 0.8 * nrow(data))
train_data <- data[split, ]
test_data <- data[-split, ] # or validation if cv
```

```{r, echo=FALSE}
library(caret)
library(class)
library(dplyr)
```
```{r}
X_train <- train_data[, colnames(train_data) != "HeartDisease"]
y_train <- train_data$HeartDisease

X_train <- data.frame(lapply(X_train, function(x) {
  if (is.character(x) || is.factor(x)) {
    as.numeric(factor(x))
  } else {
    x
  }
}))

X_test <- test_data[, colnames(test_data) != "HeartDisease"]
y_test <- test_data$HeartDisease

X_test <- data.frame(lapply(X_test, function(x) {
  if (is.character(x) || is.factor(x)) {
    as.numeric(factor(x))
  } else {
    x
  }
}))

# Cross-validation to find the best k
set.seed(123)

k_grid <- data.frame(k = 1:20)
ctrl <- trainControl(method = "cv", number = 10)

knn_cv <- train(
  x = X_train,
  y = y_train,
  method = "knn",
  tuneGrid = k_grid,
  trControl = ctrl,
  metric = "Accuracy"
)

best_k <- knn_cv$bestTune$k
cat("Best k chosen by CV:", best_k)
```
The best k was selected based on accuracy which is the proportion of correctly classified test samples.We have also the misclassification error: 1 - Accuracy
 which shows how often the model misclassifies. Under, we show the results (confusion matrix, accuracy and misclassification error) for the best K.

```{r}
y_pred <- knn(
  train = X_train,
  test = X_test,
  cl = y_train,
  k = best_k,
  prob = TRUE
)

conf_mat <- confusionMatrix(y_pred, as.factor(y_test))
print(conf_mat)

accuracy <- conf_mat$overall["Accuracy"]
cat("Accuracy:", round(accuracy, 4), "\n")

misclass_error <- 1 - accuracy
cat("Misclassification Error:", round(misclass_error, 4), "\n")
```
Now, lets find the K which gives the smallest error
```{r, echo=FALSE}
library(ggplot2)

```
```{r}
# Find the K which gives the smallest error
K <- 30
knn.error <- rep(NA, K)

set.seed(321)
for (k in 1:K) {
  knn.pred <- knn(train = X_train,
                  test = X_test,
                  cl = y_train,
                  k = k)
  
  knn.error[k] <- mean(knn.pred != y_test)
}

k_min_error <- which.min(knn.error)
min_error_value <- knn.error[k_min_error]

knn.error.df <- data.frame(k = 1:K, error = knn.error)

# Plot the error against the value of K
ggplot(knn.error.df, aes(x = k, y = error)) +
  geom_point(col = "blue") +
  geom_line(linetype = "dotted") +
  labs(title = "KNN Classification Error vs. Number of Neighbors",
       x = "Number of Neighbors (k)",
       y = "Classification Error") +
  theme_minimal()

cat("The best k for minimum error is k =", k_min_error, "with a classification error of", round(min_error_value, 4), "\n")
```


```{r}
set.seed(123)
knn <- knn(train = X_train,
             test = X_test,
             cl = y_train,
             k = k_min_error,
             prob = TRUE)

table(knn, y_test)
knn_prob <- attributes(knn)$prob

# If the prediction is "1" (heart disease), then we keep the probability associated with "1"
# If the prediction is "0" (no heart disease), we modify the probability associated with "0" to give the probability of being sick
# Since the model predicts "0" or "1", we need to reverse the probability for the "0" class (no disease)
class_1 <- which(knn == "0")
knn_prob[class_1] <- 1 - knn_prob[class_1]

# Display the probabilities for the class '1' (heart disease)
#If you want to display the probabilities, uncomment the line under "knn_prob"
#knn_prob
```
The K-Nearest Neighbors (KNN) model performed well in predicting heart disease, achieving an accuracy of about 87.5%. It correctly classified 75 healthy patients and 86 with heart disease, with 23 total misclassifications. We also examined the model’s predicted probabilities for class “1” (heart disease). High probabilities (e.g., 0.92 or 1.00) indicate strong confidence in a positive diagnosis, while low values (e.g., 0.00) suggest high confidence in a negative one. Intermediate values reflect uncertainty. These probabilities help assess how confident the model is in its predictions, which is especially useful in borderline cases.

## Logistic Regression

The logistic regression model is commonly used for binary classification problems that try to predict one out of two possible outcomes : presence or absence of heart disease. 
It estimates the probability that a given input point belongs to a particular class by estimating optimal weight of the input variables using the logistic function, which maps real-valued inputs to a range between 0 and 1. 
\[
P(y = 1 \mid x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)}}
\]

Where:

- \( P(y = 1 \mid x) \) is the probability of the output class (e.g., having heart disease),
- \( x_1, \dots, x_n \) are the input feature,
- \( \beta_0, \dots, \beta_n \) are the model parameters (learned from the data).

One of the strengths of this model is that it returns the probability of the variable to be 1, meaning that it gives more information on the reliability of the prediction. 

```{r, echo= FALSE}
model <- glm(HeartDisease ~ ., data = train_data, family = binomial)
summary(model)
```
Fitting the model on all of the input variables in the train dataset we can observe that some of them have a very high p-value (for example : 0.72 for RestingBP). Meaning that we can't reject the null hypothesis $H_0$ that the parameter associated to these variables is 0. These variables might not be useful to make the prediction model.

If we observe clear overfitting in the results it would then be smart to try the model without these irrelevant variables.

```{r, echo = FALSE}
predictions <- predict(model, test_data, type = "response")

# if the probability is > 0.5, predict heart_disease.
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_data$HeartDisease)
print(confusion_matrix)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

TN <- confusion_matrix[1,1]  # True Negatives
FP <- confusion_matrix[1,2]  # False Positives
FN <- confusion_matrix[2,1]  # False Negatives
TP <- confusion_matrix[2,2]  # True Positives

# Precision
precision <- TP / (TP + FP)
print(paste("Precision:", round(precision * 100, 2), "%"))

# Recall
recall <- TP / (TP + FN)
print(paste("Recall:", round(recall * 100, 2), "%"))
```
Using the treshold of 0.5 to decide if the model predicts a heart disease or not. We obtain by comparing the predictions of the model to the real values in the test dataset an accuracy of 89 %.

Looking at the confusion matrix, we can observe that the default of this model lies on a high amount of false positive. Meaning that there is a high precision with a lower recall. 
This is fine considering that in a heart disease detection problem it is way better to 


We can now plot the train vs error curve to see the convergence of the training error versus the test error on different sample sizes for the training.
```{r, echo=FALSE}
# train test curve
library(ggplot2)

train_sizes <- seq(0.1, 1.0, by = 0.1)  # 10% to 100% of training data
train_errors <- c()
test_errors <- c()

for (size in train_sizes) {
  # Sample a subset of the training data
  sample_indices <- sample(1:nrow(train_data), size = floor(size * nrow(train_data)))
  train_subset <- train_data[sample_indices, ]
  
  # Train the model on each subset
  model_subset <- glm(HeartDisease ~ ., data = train_subset, family = binomial)
  
  # train errors
  train_predictions <- predict(model_subset, train_subset, type = "response")
  train_predicted_classes <- ifelse(train_predictions > 0.5, 1, 0)
  train_conf_matrix <- table(Predicted = train_predicted_classes, Actual = train_subset$HeartDisease)
  train_accuracy <- sum(diag(train_conf_matrix)) / sum(train_conf_matrix)
  train_errors <- c(train_errors, 1 - train_accuracy) 

  # Test errors
  test_predictions <- predict(model_subset, test_data, type = "response")
  test_predicted_classes <- ifelse(test_predictions > 0.5, 1, 0)
  test_conf_matrix <- table(Predicted = test_predicted_classes, Actual = test_data$HeartDisease)
  test_accuracy <- sum(diag(test_conf_matrix)) / sum(test_conf_matrix)
  test_errors <- c(test_errors, 1 - test_accuracy) 
}

error_data <- data.frame(
  TrainSize = rep(train_sizes, 2),
  Error = c(train_errors, test_errors),
  Set = rep(c("Train", "Test"), each = length(train_sizes))
)

# Plot the error curve
ggplot(error_data, aes(x = TrainSize, y = Error, color = Set)) +
  geom_line() + geom_point() +
  labs(title = "Train vs. Test Error Curve", x = "Training Set Proportion", y = "Error (1 - Accuracy)") +
  theme_minimal()
```
Here, we can observe that the test error sees to converge and is much lower than the train error. The model here seems to train well without overfitting on the data as the test error remains lower.

Let's now have a look at the probability distribution that is predicted versus the actual class of each of the cases to predict.
```{r, echo=FALSE}
ggplot(test_data, aes(x = predictions, fill = factor(HeartDisease))) +
  geom_density(alpha = 0.5) +
  labs(title = "Predicted Probabilities Distribution",
       x = "Predicted Probability",
       fill = "Actual Class") +
  theme_minimal()
```
Here, we can observe that when the model tries to predict that the patient has heart disease it is often more confident about its predictions than for the opposite. 

This shows that the model is less likely to get true negatives right, as we saw previously, the recall is indeed a bit lower than the precision for this model.



## Tree based methods

In this part, we used a variety of tree-based classification methods to predict the presence of heart disease from medical data, and the ones we will present in this report are :

- Decision Trees (with and without pruning)

- Bagging

- Random Forests

- AdaBoost

Each method was applied to the same training and test dataset using the HeartDisease target variable.

**Decision Trees**

We will first show a representation of the initial tree.
The decision trees recursively split the data based on features that reduce impurity. It is easy to interpret and visualize, and it is a fast training, but there is a chance of overfitting, that is why we are going to do pruning just after.

```{r, echo=FALSE}
# Visualization of the tree
set.seed(123)
#install.packages("tree")
library(tree)
library(caret)
```
```{r}
data_tree = tree(HeartDisease~., data=train_data)
```
```{r, echo=FALSE}
plot(data_tree,type="uniform")
text(data_tree,  cex = 0.5)
#par(mfrow=c(1,2),pty="s")
tree.pred <- predict(data_tree, newdata = test_data, type = "class")
confMat <- confusionMatrix(as.factor(tree.pred), as.factor(test_data$HeartDisease))
print(confMat$table)
error_rate <- 1 - sum(diag(confMat$table)) / sum(confMat$table)
print(paste("Classification Error Rate:", round(error_rate, 4)))
```

The first error rate that we obtain with this method is not bad, we are way under 0.5, but it is still not low enough to consider that we have a good prediction. We would like the error value to be lowest possible.

After, to evoid overfitting as we said before, we are trying to prune the tree, so we are trying to see if we should stop the tree earlier to have better results, because there could be an overfitting.

```{r, echo=FALSE}
par(mfrow = c(1, 2)) 
# Cross validation
set.seed(123)
```
```{r}
cv.data <- cv.tree(data_tree,K=5)
plot(cv.data$dev ~  cv.data$size,type= "b", lwd=2, col="red", xlab="Tree Size", ylab="Deviance")
prune.data <- prune.tree(data_tree,best=7)
```
```{r, echo=FALSE}
plot(prune.data)
text(prune.data, ,  cex = 0.6)
par(mfrow = c(1, 1))
```
```{r}
# Prediction
prune.pred <- predict(prune.data, newdata = test_data, type = "class")
confMat <- confusionMatrix(as.factor(prune.pred), as.factor(test_data$HeartDisease))
print(confMat$table)
error_rate <- 1 - sum(diag(confMat$table)) / sum(confMat$table)
print(paste("Classification Error Rate:", round(error_rate, 4)))
```
Indeed, a tree with 7 leaves performs best and we see that the error is lower than with the normal tree.

**Decision Trees with Gini vs. Entropy Splitting**

Now, we will split the data with the cross entropy and after the Gini index. The cross entropy will give more pure nodes but it can lead to bigger trees, the Gini split goes faster.

```{r}
tree.data <- tree(HeartDisease ~ ., data = train_data, split = "deviance") # cross entropy
```
```{r, echo=FALSE}
#summary(tree.data)
#plot(tree.data,type="proportional")
#text(tree.data,pretty=1)
```
```{r}
tree.datag <- tree(HeartDisease ~ ., data = train_data, split = "gini") # Gini index
```
```{r, echo=FALSE}
#summary(tree.datag)
#plot(tree.datag,type="proportional")
#text(tree.datag,pretty=1)
```
The number of terminal nodes for the cross entropy splitting is 12 and for the Gini index it is 73, which is a lot, this could be overfitting.

```{r, echo=FALSE}
library(caret)
```
```{r}
# Prediction for cross entropy
tree_pred <- predict(tree.data, test_data, type="class")
confMat <- confusionMatrix(tree_pred, reference=test_data$HeartDisease)
print(confMat$table)
error_rate <- 1 - sum(diag(confMat$table)) / sum(confMat$table)
print(paste("Classification Error Rate:", round(error_rate, 4)))
# Prediction for gini index
tree_predg <- predict(tree.datag, test_data, type="class")
confMatg <- confusionMatrix(tree_predg, reference=test_data$HeartDisease)
print(confMatg$table)
error_rate <- 1 - sum(diag(confMatg$table)) / sum(confMatg$table)
print(paste("Classification Error Rate:", round(error_rate, 4)))
```
Indeed, now we see that the prediction is better for the split with the cross entropy method than for the Gini index.

Now, we will use more complex methods like Adaboost, the bagging and random forest methods to try having the lowest error possible.

**Bagging**

Bagging generates multiple bootstrap samples, fits a tree on each, and aggregates predictions via majority voting. It reduces variance and handles overfitting better than a single tree, as we have seen before, but it is more difficult to interpret and it requires more computation.

```{r, echo=FALSE}
library(randomForest)
set.seed(123)
```
```{r}
bag_model <- randomForest(HeartDisease ~ ., data = train_data,
                          mtry = 10, ntree = 500, importance = TRUE)

# confusion matrix
bag_pred <- predict(bag_model, newdata = test_data)
conf_bag <- table(bag_pred, test_data$HeartDisease)
```
```{r, echo=FALSE}
print(conf_bag)
err_bag <- 1 - sum(diag(conf_bag)) / sum(conf_bag)
cat("Classification error (Bagging) :", round(err_bag, 4), "\n")
```

**Random Forest**

Random forest is a particular case of bagging, but each tree considers only a random subset of features when splitting. As a consequence, there is less correlation among trees, so there is a better performance, and it handles high-dimensional data well. As Random forest, it has as a weakness a difficult interpretation and moreover, it can be slow with many trees.

```{r}
set.seed(123)
rf_model <- randomForest(HeartDisease ~ ., data = train_data,
                         mtry = 3, ntree = 500, importance = TRUE)

# Confusion matrix
rf_pred <- predict(rf_model, newdata = test_data)
conf_rf <- table(rf_pred, test_data$HeartDisease)
```
```{r, echo=FALSE}
print(conf_rf)
err_rf <- 1 - sum(diag(conf_rf)) / sum(conf_rf)
cat("Classification error (Random Forest) :", round(err_rf, 4), "\n")
```

Both of the methods are giving a lower error value than previously, but the random forest is still better.

**AdaBoost**

We will finally use the Ada Boost method. It focuses on misclassified points by reweighting the dataset at each iteration to improve the weak learners. It has good performance on complex datasets and it reduces both bias and variance. But it can be sensitive to noisy data and it is quite slow.

```{r, echo=FALSE}
library(ada)
library(caret)
set.seed(123)
```
```{r}
r.ada <-ada(HeartDisease~.,train_data,iter=400,type="discrete",loss="ada",control=rpart.control())
#plot(r.ada)
#ada boost test error
ada.pred <- predict(r.ada,newdata=test_data)
confMat_ada <- confusionMatrix(as.factor(ada.pred), as.factor(test_data$HeartDisease))
print(confMat_ada$table)
error_ada <- 1 - sum(diag(confMat_ada$table)) / sum(confMat_ada$table)
print(error_ada)
```

Ada Boost gives a low error but the random forest method is a little bit better, maybe it is because our dataset in this analyse is not so complex.

To conclude on all the tree methods that we tried, the random forest is the one that can predict the lower error.

```{r, echo=FALSE}
par(mfrow = c(1, 2))
varImpPlot(rf_model, main = "Random Forest", pch = 20)
par(mfrow = c(1, 1))
```

And here are the more important parameters to predict the heart disease fro the tree method which has the lowest error. It seems that ST slope is the most important factor to predict the heart disease, then Cholesterol and type of the chest pain are also very important.

```{r parameter tuning, results='hide', echo=FALSE}
library(gbm) # for original implementation of regular and stochastic GBMs
library(rsample) #to subsample training and test sets

#run a basic GBM model
set.seed(123) # for reproducibility
heart_gbm1 <-gbm(formula= HeartDisease ~ .,
                data= train_data, distribution= "gaussian", #SSE loss function
            n.trees= 3000,
            shrinkage= 0.1, #learning rate
            interaction.depth= 3,
            n.minobsinnode= 10, #minimual number of observations interminal nodes
            cv.folds= 10,
            bag.fraction=1 #The default is 0.5, but here we want to have all data in each iteration
            )
(best <-which.min(heart_gbm1$cv.error))
sqrt(heart_gbm1$cv.error[best])
#gbm.perf(heart_gbm1, method = "cv")
  
set.seed(123) # for reproducibility
heart_gbm2 <-gbm(formula= HeartDisease ~ ., data= train_data,
                 distribution= "gaussian", #SSE loss function
                 n.trees= 3000,
                 shrinkage= 0.1, #learning rate
                 interaction.depth= 3,
                 n.minobsinnode= 10, #minimualnumber of observationsinterminalnodes
                 cv.folds= 10,
                 bag.fraction=0.5)
```

```{r xgboost, results='hide', echo=FALSE}
library(xgboost)
library(recipes)

#Training data and response
xgb_prep<- recipe(HeartDisease ~ ., data = train_data) %>%
  step_integer(all_nominal()) %>%
  prep(training = train_data, retain = TRUE) %>%
  juice()

X <-as.matrix(xgb_prep[setdiff(names(xgb_prep),"HeartDisease")])
Y <-xgb_prep$HeartDisease

#Test data and test response
xgb_prep_test <- recipe(HeartDisease ~ ., data = test_data) %>%
  step_integer(all_nominal()) %>%
  prep(training = test_data, retain = TRUE) %>%
  juice()
X_test <-as.matrix(xgb_prep_test[setdiff(names(xgb_prep_test), "HeartDisease")])
Y_test <-xgb_prep_test$HeartDisease
 
set.seed(123)
heart_xgb <-xgboost(data= X, label= Y,
                    nrounds= 6000,
                    objective= "reg:squarederror",
                    early_stopping_rounds= 50,
                    params= list(eta= 0.01, #learningrate(wecall it$\nu$)
                                 lambda=0.1, #L2regularization te
                                 max_depth= 6,
                                 min_child_weight= 3,
                                 subsample= 0.8, #Proportion of data pointsusedforeachtree
                                 colsample_bytree= 0.5, #subsample ratioofcolumnswhen constructingonetree
                                 nthread=12),
                    verbose= 0)
X.pred.test <-predict(heart_xgb,newdata= X_test)
sqrt(sum((X.pred.test-Y_test)^2)/nrow(X_test))
```

We tried the parameters tuning and the XGBoost too, but the results were not good, with an error around 0.3.

# Results and interpretation

Table of comparison of all the misclassification error :


| Method              | Misclassification error |
|---------------------|-------------------------|
| KNN                 | 0.1359                  |
| Logistic Regression | 0.1087                  |
| Decision Tree       | 0.1739                  |
| Pruning Tree        | 0.1685                  |
| Entropy             | 0.1739                  |
| Gini                | 0.2554                  |
| Bagging             | 0.1576                  |
| Random Forest       | 0.1413                  |
| AdaBoost            | 0.1576                  |

Based on misclassification error, Logistic Regression performed the best with an error of 0.108, followed closely by K-Nearest Neighbors (KNN) at 0.1359, and Random Forest at 0.1413. These methods show strong predictive performance for the classification task. Ensemble methods like Bagging and AdaBoost had slightly higher errors (~0.1576), while single decision trees (including Entropy, Gini, and Pruned Tree) generally underperformed, with Gini showing the worst result at 0.2554. In terms of computational cost, Logistic Regression is lightweight and interpretable, whereas KNN requires more memory and time for large datasets. Tree-based methods and ensembles are more flexible and better at capturing complex relationships but can be computationally heavier and less interpretable. Overall, Logistic Regression offered the best balance between simplicity, performance, and generalizability in this case.


We are going to see a representation of all the ROC curves together.
```{r, echo=FALSE}
library(pROC)
par(mar = c(5, 5, 4, 8), xpd = TRUE)
tree.prob <- predict(data_tree, newdata = test_data, type = "vector")[,2]  #
roc.tree <- roc(test_data$HeartDisease, tree.prob)

prune.prob <- predict(prune.data, newdata = test_data, type = "vector")[,2]
roc.prune <- roc(test_data$HeartDisease, prune.prob)

bag.prob <- predict(bag_model, newdata = test_data, type = "prob")[,2]
roc.bag <- roc(test_data$HeartDisease, bag.prob)

rf.prob <- predict(rf_model, newdata = test_data, type = "prob")[,2]
roc.rf <- roc(test_data$HeartDisease, rf.prob)

ada.prob <- predict(r.ada, newdata = test_data, type = "prob")[,2]
roc.ada <- roc(test_data$HeartDisease, ada.prob)

glm.prob <- predict(tree.data, test_data, type="vector")[,2]
roc.glm <- roc(test_data$HeartDisease, glm.prob)

plot(roc.tree, col = "blue", lwd = 2, main = "ROC curves - Comparison of methods")

lines(roc.prune, col = "darkgreen", lwd = 2)
lines(roc.bag, col = "orange", lwd = 2)
lines(roc.rf, col = "red", lwd = 2)
lines(roc.ada, col = "purple", lwd = 2)
lines(roc.glm, col = "yellow", lwd = 2)

legend("topright", inset = c(-0.35, 0), legend = c("Simple tree", "Pruning tree", "Bagging", "Random Forest", "AdaBoost", "Logistic"),
       col = c("blue", "darkgreen", "orange", "red", "purple", "yellow"),
       lwd = 2, bty = "n", cex = 0.9)
par(mar = c(5, 4, 4, 2) + 0.1)
cat("AUC Simple Tree :", auc(roc.tree), "\n")
cat("AUC Pruning tree :", auc(roc.prune), "\n")
cat("AUC Bagging :", auc(roc.bag), "\n")
cat("AUC Random Forest :", auc(roc.rf), "\n")
cat("AUC AdaBoost :", auc(roc.ada), "\n")
cat("AUC Logistic :", auc(roc.glm), "\n")
```

The most important parameters are as a consequence :

Limitations : 

# Summary