---
title: "Compulsory Exercise 2: Compulsory Exercise 2: Study of Heart Disease"
author:
- Adam DJAROUD
- Antoine REILHES
- Flavie BERTRAND
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: 
  - \usepackage{amsmath}
  - \geometry{a4paper, margin=0.7in}
  - \usepackage{titlesec}
  - \titlespacing{\section}{0pt}{1pt}{1pt}
  - \titlespacing{\subsection}{0pt}{1pt}{1pt} 
output:
  # html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "The objective of this project is to identify the most important medical parameters that help predict whether an individual has a heart disease or not. This question is important concerning early detection and intervention of the disease. To respond to this purpose, we worked with a dataset of 918 observations and a variety of medical parameters. We applied and compared several classification models, including the K nearest neighbors, logistic regression and trees methods. We evaluated the methods using classification error rates, confusion matrices, and ROC curves to measure and compare performance.
In this study, the best model was able to predict heart disease with 89% accuracy using medical data. These results are highly significant to healthcare professionals and data scientists, as machine learning methods are shown here to be able to provide a reliable framework for detection of heart disease. 
This study hints towards the best methods to further investigate in order to have the best predictions in the future."
---
  
```{r setup, include=FALSE}
library(knitr)
# Feel free to change the setting as you see fit
knitr::opts_chunk$set(echo = TRUE,
                      tidy = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      strip.white = TRUE,
                      prompt = FALSE,
                      cache = TRUE,
                      size = "scriptsize",
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = "center")

```

```{r, eval=TRUE, echo=FALSE}
library("knitr")
library("rmarkdown")
```


# Introduction

The problem of our project is to determine important parameters to know if someone has a heart disease. To do it, we will use a dataset of 918 observations, with different parameters such as the age, the cholesterol rate, the shape of the ST segment and other medical parameters. By studying this data, we hope to build a pre-diagnosis model that is able to evaluate if a person is likely to have a heart disease, to then be able to send him to specialised cardiologists for further diagnosis.
The data set we used is provided by user Fedesoriano on Kaggle.
This problem enters into the scope of supervised learning where the goal is to classify a binary output variable, here heart_disease. Classification machine learning methods typically used for binary classification will be compared to find the best predictor for this variable.
The target audience for such models would be doctors, because the results could help them diagnose heart disease. It is also trying to discover the relations between heart disease and all the other parameters of the dataset which could be useful for further medical research on the causes of heart disease.

# Descriptive data analysis/statistics

First, we will pre process the data by standardizing the numeric columns, the dataset was clean, without missing values.

```{r, echo=FALSE}
library(readr)
data <- read.csv("./heart.csv")
#head(data)
#summary(data)
set.seed(123)
```


```{r}
# standardization of numeric columns
numeric_columns <- sapply(data, is.numeric)
numeric_columns["HeartDisease"] <- FALSE
print(numeric_columns)
data[numeric_columns] <- scale(data[numeric_columns])
summary(data)
```
Here is a summary of the data, where we can see measures such as mean, median and range.

```{r}
data[] <- lapply(data, function(x) {
  if (is.character(x) || is.factor(x)) {
    as.numeric(factor(x)) - 1  
  } else {
    x
  }
})
#str(data)
```

In this dataset, there are 12 columns, so 12 parameters and 918 rows, so 918 observations. We have quantitative parameters, which are : Age, RestingBP, Cholesterol, MaxHR and OldPeak. The qualitative parameters are Sex, ChestPainType, FastingBS, RestingECG, ExerciseAngina, ST_Slope and HeartDisease.
Here is a concrete explanation of the meaning of all the parameters of this dataset :

- Age : Age (in years)

- RestingBP : Resting Blood Pressure, measured in mmHg, taken while the patient is at rest.

- Cholesterol : Serum Cholesterol in mg/dL.

- MaxHR : Maximum Heart Rate Achieved during exercise stress test.

- OldPeak : ST Depression Induced by Exercise Relative to Rest.

- Sex : binary.

- ChestPainType : Type of Chest Pain with categories Typical Angina, Atypical Angina, Non-anginal Pain and Asymptomatic.

- FastingBS : Fasting Blood Sugar > 120 mg/dL which is binary.

- RestingECG : Resting Electrocardiogram Results.

- ExerciseAngina : Exercise-Induced Angina which is binary.

- ST_Slope : Slope of the Peak Exercise ST Segment which can be Upsloping, Flat or Downsloping.

- HeartDisease : Presence of Heart Disease, which is binary.


Here are histograms for the qualitative variables.
```{r, fig.width=9, fig.height=6, echo=FALSE}
library(ggplot2)
library(gridExtra)

plot1 <- ggplot(data, aes(x = factor(Sex))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Heart disease", x = "Heart disease", y = "Number") +
  theme_minimal()

plot2 <- ggplot(data, aes(x = ChestPainType, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "Sex depending HD",
       x = "Sex",
       fill = "HD") +
  theme_minimal()

plot3 <- ggplot(data, aes(x = Sex, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "Type of thoracic pain depending HD",
       x = "Type of thoracic pain",
       fill = "HD") +
  theme_minimal()

plot4 <- ggplot(data, aes(x = FastingBS, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "FastingBS depending HD",
       x = "FastingBS",
       fill = "HD") +
  theme_minimal()

plot5 <- ggplot(data, aes(x = RestingECG, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "RestingECG depending HD",
       x = "RestingECG",
       fill = "HD") +
  theme_minimal()

plot6 <- ggplot(data, aes(x = ExerciseAngina, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "ExerciseAngina depending HD",
       x = "ExerciseAngina",
       fill = "HD") +
  theme_minimal()

plot7 <- ggplot(data, aes(x = ST_Slope, fill = factor(HeartDisease))) +
  geom_bar(position = "dodge") +
  labs(title = "ST_Slope depending HD",
       x = "ST_Slope",
       fill = "HD") +
  theme_minimal()

library(patchwork)
reduce_text_size <- function(p, size = 8) {
  p + theme(title = element_text(size = size),
            axis.title.x = element_text(size = size - 1),
            axis.title.y = element_text(size = size - 1),
            axis.text.x = element_text(size = size - 2),
            axis.text.y = element_text(size = size - 2),
            legend.text = element_text(size = size - 2),
            legend.title = element_text(size = size - 2))
}

plot1_small <- reduce_text_size(plot1)
plot2_small <- reduce_text_size(plot2)
plot3_small <- reduce_text_size(plot3)
plot4_small <- reduce_text_size(plot4)
plot5_small <- reduce_text_size(plot5)
plot6_small <- reduce_text_size(plot6)
plot7_small <- reduce_text_size(plot7)


plot_combined_three_col <- plot1_small + plot2_small + plot3_small +
  plot4_small + plot5_small + plot6_small + plot7_small +
  plot_layout(ncol = 4, widths = c(1, 1, 1))

plot_combined_three_col
```


And here are the distribution and the correlation between quantitative variables.
```{r, echo=FALSE}
library(GGally)
library(ggplot2)
```
```{r}
pm <- ggpairs(data,
              columns = c("Age", "RestingBP", "Cholesterol", "MaxHR", "Oldpeak"), 
              mapping = aes(color = factor(HeartDisease)))
print(pm)
```
There seems to be no correlation between two variables, it means that changes in one variable do not systematically correspond to changes in the other, they vary independently of each other.


## KNN

In this part, we will use the KNN method. KNN is a non-parametric method where a data point is classified based on the majority vote of its K nearest neighbors in the feature space. The choice of K is crucial for performance. This is why the first thing is to find the best K

```{r}
data$HeartDisease <- as.factor(data$HeartDisease)
split <- sample(1:nrow(data), size = 0.8 * nrow(data))
train_data <- data[split, ]
test_data <- data[-split, ] # or validation if cv
```

```{r, echo=FALSE}
library(caret)
library(class)
library(dplyr)
```
```{r}
X_train <- train_data[, colnames(train_data) != "HeartDisease"]
y_train <- train_data$HeartDisease

X_train <- data.frame(lapply(X_train, function(x) {
  if (is.character(x) || is.factor(x)) {
    as.numeric(factor(x))
  } else {
    x
  }
}))

X_test <- test_data[, colnames(test_data) != "HeartDisease"]
y_test <- test_data$HeartDisease

X_test <- data.frame(lapply(X_test, function(x) {
  if (is.character(x) || is.factor(x)) {
    as.numeric(factor(x))
  } else {
    x
  }
}))

# Cross-validation to find the best k
set.seed(123)

k_grid <- data.frame(k = 1:20)
ctrl <- trainControl(method = "cv", number = 10)

knn_cv <- train(
  x = X_train,
  y = y_train,
  method = "knn",
  tuneGrid = k_grid,
  trControl = ctrl,
  metric = "Accuracy"
)

best_k <- knn_cv$bestTune$k
cat("Best k chosen by CV:", best_k)
```
The best k was selected based on accuracy which is the proportion of correctly classified test samples.We have also the misclassification error: 1 - Accuracy
 which shows how often the model misclassifies. Under, we show the results (confusion matrix, accuracy and misclassification error) for the best K.

```{r}
y_pred <- knn(
  train = X_train,
  test = X_test,
  cl = y_train,
  k = best_k,
  prob = TRUE
)

conf_mat <- confusionMatrix(y_pred, as.factor(y_test))
print(conf_mat)

accuracy <- conf_mat$overall["Accuracy"]
cat("Accuracy:", round(accuracy, 4), "\n")

misclass_error <- 1 - accuracy
cat("Misclassification Error:", round(misclass_error, 4), "\n")
```
Now, lets find the K which gives the smallest error
```{r, echo=FALSE}
library(ggplot2)

```
```{r}
# Find the K which gives the smallest error
K <- 30
knn.error <- rep(NA, K)

set.seed(321)
for (k in 1:K) {
  knn.pred <- knn(train = X_train,
                  test = X_test,
                  cl = y_train,
                  k = k)
  
  knn.error[k] <- mean(knn.pred != y_test)
}

k_min_error <- which.min(knn.error)
min_error_value <- knn.error[k_min_error]

knn.error.df <- data.frame(k = 1:K, error = knn.error)

# Plot the error against the value of K
ggplot(knn.error.df, aes(x = k, y = error)) +
  geom_point(col = "blue") +
  geom_line(linetype = "dotted") +
  labs(title = "KNN Classification Error vs. Number of Neighbors",
       x = "Number of Neighbors (k)",
       y = "Classification Error") +
  theme_minimal()

cat("The best k for minimum error is k =", k_min_error, "with a classification error of", round(min_error_value, 4), "\n")
```


```{r, results='hide'}
set.seed(123)
knn <- knn(train = X_train,
             test = X_test,
             cl = y_train,
             k = k_min_error,
             prob = TRUE)

table(knn, y_test)
knn_prob <- attributes(knn)$prob

# If the prediction is "1" (heart disease), then we keep the probability associated with "1"
# If the prediction is "0" (no heart disease), we modify the probability associated with "0" to give the probability of being sick
# Since the model predicts "0" or "1", we need to reverse the probability for the "0" class (no disease)
class_1 <- which(knn == "0")
knn_prob[class_1] <- 1 - knn_prob[class_1]

# Display the probabilities for the class '1' (heart disease)
#If you want to display the probabilities, uncomment the line under "knn_prob"
#knn_prob
```
The K-Nearest Neighbors (KNN) model performed well in predicting heart disease, achieving an accuracy of about 87.5%. It correctly classified 75 healthy patients and 86 with heart disease, with 23 total misclassifications. We also examined the model’s predicted probabilities for class “1” (heart disease). High probabilities (e.g., 0.92 or 1.00) indicate strong confidence in a positive diagnosis, while low values (e.g., 0.00) suggest high confidence in a negative one. Intermediate values reflect uncertainty. These probabilities help assess how confident the model is in its predictions, which is especially useful in borderline cases.

## Logistic Regression

The logistic regression model is commonly used for binary classification problems that try to predict one out of two possible outcomes : presence or absence of heart disease. 
It estimates the probability that a given input point belongs to a particular class by estimating optimal weight of the input variables using the logistic function, which maps real-valued inputs to a range between 0 and 1. 
\[
P(y = 1 \mid x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)}}
\]

Where:

- \( P(y = 1 \mid x) \) is the probability of the output class (e.g., having heart disease),
- \( x_1, \dots, x_n \) are the input feature,
- \( \beta_0, \dots, \beta_n \) are the model parameters (learned from the data).

One of the strengths of this model is that it returns the probability of the variable to be 1, meaning that it gives more information on the reliability of the prediction. 

```{r, echo= FALSE}
model <- glm(HeartDisease ~ ., data = train_data, family = binomial)
summary(model)
```
Fitting the model on all of the input variables in the train dataset we can observe that some of them have a very high p-value (for example : 0.72 for RestingBP). Meaning that we can't reject the null hypothesis $H_0$ that the parameter associated to these variables is 0. These variables might not be useful to make the prediction model.

If we observe clear overfitting in the results it would then be smart to try the model without these irrelevant variables.

```{r, echo = FALSE}
predictions <- predict(model, test_data, type = "response")

# if the probability is > 0.5, predict heart_disease.
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = test_data$HeartDisease)
print(confusion_matrix)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

TN <- confusion_matrix[1,1]  # True Negatives
FP <- confusion_matrix[1,2]  # False Positives
FN <- confusion_matrix[2,1]  # False Negatives
TP <- confusion_matrix[2,2]  # True Positives

# Precision
precision <- TP / (TP + FP)
print(paste("Precision:", round(precision * 100, 2), "%"))

# Recall
recall <- TP / (TP + FN)
print(paste("Recall:", round(recall * 100, 2), "%"))
```
Using the treshold of 0.5 to decide if the model predicts a heart disease or not. We obtain by comparing the predictions of the model to the real values in the test dataset an accuracy of 89 %.

Looking at the confusion matrix, we can observe that the default of this model lies on a high amount of false positive. Meaning that there is a high precision with a lower recall. 
This is fine considering that in a heart disease detection problem it is way better to 


We can now plot the train vs error curve to see the convergence of the training error versus the test error on different sample sizes for the training.
```{r, echo=FALSE}
# train test curve
library(ggplot2)

train_sizes <- seq(0.1, 1.0, by = 0.1)  # 10% to 100% of training data
train_errors <- c()
test_errors <- c()

for (size in train_sizes) {
  # Sample a subset of the training data
  sample_indices <- sample(1:nrow(train_data), size = floor(size * nrow(train_data)))
  train_subset <- train_data[sample_indices, ]
  
  # Train the model on each subset
  model_subset <- glm(HeartDisease ~ ., data = train_subset, family = binomial)
  
  # train errors
  train_predictions <- predict(model_subset, train_subset, type = "response")
  train_predicted_classes <- ifelse(train_predictions > 0.5, 1, 0)
  train_conf_matrix <- table(Predicted = train_predicted_classes, Actual = train_subset$HeartDisease)
  train_accuracy <- sum(diag(train_conf_matrix)) / sum(train_conf_matrix)
  train_errors <- c(train_errors, 1 - train_accuracy) 

  # Test errors
  test_predictions <- predict(model_subset, test_data, type = "response")
  test_predicted_classes <- ifelse(test_predictions > 0.5, 1, 0)
  test_conf_matrix <- table(Predicted = test_predicted_classes, Actual = test_data$HeartDisease)
  test_accuracy <- sum(diag(test_conf_matrix)) / sum(test_conf_matrix)
  test_errors <- c(test_errors, 1 - test_accuracy) 
}

error_data <- data.frame(
  TrainSize = rep(train_sizes, 2),
  Error = c(train_errors, test_errors),
  Set = rep(c("Train", "Test"), each = length(train_sizes))
)

# Plot the error curve
ggplot(error_data, aes(x = TrainSize, y = Error, color = Set)) +
  geom_line() + geom_point() +
  labs(title = "Train vs. Test Error Curve", x = "Training Set Proportion", y = "Error (1 - Accuracy)") +
  theme_minimal()
```
Here, we can observe that the test error sees to converge and is much lower than the train error. The model here seems to train well without overfitting on the data as the test error remains lower.

Let's now have a look at the probability distribution that is predicted versus the actual class of each of the cases to predict.
```{r, echo=FALSE}
ggplot(test_data, aes(x = predictions, fill = factor(HeartDisease))) +
  geom_density(alpha = 0.5) +
  labs(title = "Predicted Probabilities Distribution",
       x = "Predicted Probability",
       fill = "Actual Class") +
  theme_minimal()
```
Here, we can observe that when the model tries to predict that the patient has heart disease it is often more confident about its predictions than for the opposite. 

This shows that the model is less likely to get true negatives right, as we saw previously, the recall is indeed a bit lower than the precision for this model.



## Tree based methods

In this part, we used a variety of tree-based classification methods to predict the presence of heart disease from medical data, and the ones we will present in this report are :

- Decision Trees (with and without pruning)

- Bagging

- Random Forests

- AdaBoost

Each method was applied to the same training and test dataset using the HeartDisease target variable.

**Decision Trees**

We will first show a representation of the initial tree.
The decision trees recursively split the data based on features that reduce impurity. It is easy to interpret and visualize, and it is a fast training, but there is a chance of overfitting, that is why we are going to do pruning just after.

```{r, echo=FALSE}
# Visualization of the tree
set.seed(123)
#install.packages("tree")
library(tree)
library(caret)
```
```{r}
data_tree = tree(HeartDisease~., data=train_data)
```
```{r, echo=FALSE}
plot(data_tree,type="uniform")
text(data_tree,  cex = 0.5)
#par(mfrow=c(1,2),pty="s")
tree.pred <- predict(data_tree, newdata = test_data, type = "class")
confMat <- confusionMatrix(as.factor(tree.pred), as.factor(test_data$HeartDisease))
print(confMat$table)
error_rate <- 1 - sum(diag(confMat$table)) / sum(confMat$table)
print(paste("Classification Error Rate:", round(error_rate, 4)))
```

The first error rate that we obtain with this method is not bad, we are way under 0.5, but it is still not low enough to consider that we have a good prediction. We would like the error value to be lowest possible.

After, to evoid overfitting as we said before, we are trying to prune the tree, so we are trying to see if we should stop the tree earlier to have better results, because there could be an overfitting.

```{r, echo=FALSE}
par(mfrow = c(1, 2)) 
# Cross validation
set.seed(123)
```
```{r}
cv.data <- cv.tree(data_tree,K=5)
plot(cv.data$dev ~  cv.data$size,type= "b", lwd=2, col="red", xlab="Tree Size", ylab="Deviance")
prune.data <- prune.tree(data_tree,best=7)
```
```{r, echo=FALSE}
plot(prune.data)
text(prune.data, ,  cex = 0.6)
par(mfrow = c(1, 1))
```
```{r}
# Prediction
prune.pred <- predict(prune.data, newdata = test_data, type = "class")
confMat <- confusionMatrix(as.factor(prune.pred), as.factor(test_data$HeartDisease))
print(confMat$table)
error_rate <- 1 - sum(diag(confMat$table)) / sum(confMat$table)
print(paste("Classification Error Rate:", round(error_rate, 4)))
```
Indeed, a tree with 7 leaves performs best and we see that the error is lower than with the normal tree.

**Decision Trees with Gini vs. Entropy Splitting**

Now, we will split the data with the cross entropy and after the Gini index. The cross entropy will give more pure nodes but it can lead to bigger trees, the Gini split goes faster.

```{r}
tree.data <- tree(HeartDisease ~ ., data = train_data, split = "deviance") # cross entropy
```
```{r, echo=FALSE}
#summary(tree.data)
#plot(tree.data,type="proportional")
#text(tree.data,pretty=1)
```
```{r}
tree.datag <- tree(HeartDisease ~ ., data = train_data, split = "gini") # Gini index
```
```{r, echo=FALSE}
#summary(tree.datag)
#plot(tree.datag,type="proportional")
#text(tree.datag,pretty=1)
```
The number of terminal nodes for the cross entropy splitting is 12 and for the Gini index it is 73, which is a lot, this could be overfitting.

```{r, echo=FALSE}
library(caret)
```
```{r}
# Prediction for cross entropy
tree_pred <- predict(tree.data, test_data, type="class")
confMat <- confusionMatrix(tree_pred, reference=test_data$HeartDisease)
print(confMat$table)
error_rate <- 1 - sum(diag(confMat$table)) / sum(confMat$table)
print(paste("Classification Error Rate:", round(error_rate, 4)))
# Prediction for gini index
tree_predg <- predict(tree.datag, test_data, type="class")
confMatg <- confusionMatrix(tree_predg, reference=test_data$HeartDisease)
print(confMatg$table)
error_rate <- 1 - sum(diag(confMatg$table)) / sum(confMatg$table)
print(paste("Classification Error Rate:", round(error_rate, 4)))
```
Indeed, now we see that the prediction is better for the split with the cross entropy method than for the Gini index.

Now, we will use more complex methods like Adaboost, the bagging and random forest methods to try having the lowest error possible.

**Bagging**

Bagging generates multiple bootstrap samples, fits a tree on each, and aggregates predictions via majority voting. It reduces variance and handles overfitting better than a single tree, as we have seen before, but it is more difficult to interpret and it requires more computation.

```{r, echo=FALSE}
library(randomForest)
set.seed(123)
```
```{r}
bag_model <- randomForest(HeartDisease ~ ., data = train_data,
                          mtry = 10, ntree = 500, importance = TRUE)

# confusion matrix
bag_pred <- predict(bag_model, newdata = test_data)
conf_bag <- table(bag_pred, test_data$HeartDisease)
```
```{r, echo=FALSE}
print(conf_bag)
err_bag <- 1 - sum(diag(conf_bag)) / sum(conf_bag)
cat("Classification error (Bagging) :", round(err_bag, 4), "\n")
```

**Random Forest**

Random forest is a particular case of bagging, but each tree considers only a random subset of features when splitting. As a consequence, there is less correlation among trees, so there is a better performance, and it handles high-dimensional data well. As Random forest, it has as a weakness a difficult interpretation and moreover, it can be slow with many trees.

```{r}
set.seed(123)
rf_model <- randomForest(HeartDisease ~ ., data = train_data,
                         mtry = 3, ntree = 500, importance = TRUE)

# Confusion matrix
rf_pred <- predict(rf_model, newdata = test_data)
conf_rf <- table(rf_pred, test_data$HeartDisease)
```
```{r, echo=FALSE}
print(conf_rf)
err_rf <- 1 - sum(diag(conf_rf)) / sum(conf_rf)
cat("Classification error (Random Forest) :", round(err_rf, 4), "\n")
```

Both of the methods are giving a lower error value than previously, but the random forest is still better.

**AdaBoost**

We will finally use the Ada Boost method. It focuses on misclassified points by reweighting the dataset at each iteration to improve the weak learners. It has good performance on complex datasets and it reduces both bias and variance. But it can be sensitive to noisy data and it is quite slow.

```{r, echo=FALSE}
library(ada)
library(caret)
set.seed(123)
```
```{r}
r.ada <-ada(HeartDisease~.,train_data,iter=400,type="discrete",loss="ada",control=rpart.control())
#plot(r.ada)
#ada boost test error
ada.pred <- predict(r.ada,newdata=test_data)
confMat_ada <- confusionMatrix(as.factor(ada.pred), as.factor(test_data$HeartDisease))
print(confMat_ada$table)
error_ada <- 1 - sum(diag(confMat_ada$table)) / sum(confMat_ada$table)
print(error_ada)
```

Ada Boost gives a low error but the random forest method is a little bit better, maybe it is because our dataset in this analyse is not so complex.

To conclude on all the tree methods that we tried, the random forest is the one that can predict the lower error.

```{r, echo=FALSE}
par(mfrow = c(1, 2))
varImpPlot(rf_model, main = "Random Forest", pch = 20)
par(mfrow = c(1, 1))
```

And here are the more important parameters to predict the heart disease fro the tree method which has the lowest error. It seems that ST slope is the most important factor to predict the heart disease, then Cholesterol and type of the chest pain are also very important.

```{r parameter tuning, results='hide', echo=FALSE}
library(gbm) # for original implementation of regular and stochastic GBMs
library(rsample) #to subsample training and test sets

#run a basic GBM model
set.seed(123) # for reproducibility
heart_gbm1 <-gbm(formula= HeartDisease ~ .,
                data= train_data, distribution= "gaussian", #SSE loss function
            n.trees= 3000,
            shrinkage= 0.1, #learning rate
            interaction.depth= 3,
            n.minobsinnode= 10, #minimual number of observations interminal nodes
            cv.folds= 10,
            bag.fraction=1 #The default is 0.5, but here we want to have all data in each iteration
            )
(best <-which.min(heart_gbm1$cv.error))
sqrt(heart_gbm1$cv.error[best])
#gbm.perf(heart_gbm1, method = "cv")
  
set.seed(123) # for reproducibility
heart_gbm2 <-gbm(formula= HeartDisease ~ ., data= train_data,
                 distribution= "gaussian", #SSE loss function
                 n.trees= 3000,
                 shrinkage= 0.1, #learning rate
                 interaction.depth= 3,
                 n.minobsinnode= 10, #minimualnumber of observationsinterminalnodes
                 cv.folds= 10,
                 bag.fraction=0.5)
```

```{r xgboost, results='hide', echo=FALSE}
library(xgboost)
library(recipes)

#Training data and response
xgb_prep<- recipe(HeartDisease ~ ., data = train_data) %>%
  step_integer(all_nominal()) %>%
  prep(training = train_data, retain = TRUE) %>%
  juice()

X <-as.matrix(xgb_prep[setdiff(names(xgb_prep),"HeartDisease")])
Y <-xgb_prep$HeartDisease

#Test data and test response
xgb_prep_test <- recipe(HeartDisease ~ ., data = test_data) %>%
  step_integer(all_nominal()) %>%
  prep(training = test_data, retain = TRUE) %>%
  juice()
X_test <-as.matrix(xgb_prep_test[setdiff(names(xgb_prep_test), "HeartDisease")])
Y_test <-xgb_prep_test$HeartDisease
 
set.seed(123)
heart_xgb <-xgboost(data= X, label= Y,
                    nrounds= 6000,
                    objective= "reg:squarederror",
                    early_stopping_rounds= 50,
                    params= list(eta= 0.01, #learningrate(wecall it$\nu$)
                                 lambda=0.1, #L2regularization te
                                 max_depth= 6,
                                 min_child_weight= 3,
                                 subsample= 0.8, #Proportion of data pointsusedforeachtree
                                 colsample_bytree= 0.5, #subsample ratioofcolumnswhen constructingonetree
                                 nthread=12),
                    verbose= 0)
X.pred.test <-predict(heart_xgb,newdata= X_test)
sqrt(sum((X.pred.test-Y_test)^2)/nrow(X_test))
```

We tried the parameters tuning and the XGBoost too, but the results were not good, with an error around 0.3.

# Results and interpretation

Table of comparison of all the misclassification error :


| Method              | Misclassification error |
|---------------------|-------------------------|
| KNN                 | 0.1359                  |
| Logistic Regression | 0.1087                  |
| Decision Tree       | 0.1739                  |
| Pruning Tree        | 0.1685                  |
| Entropy             | 0.1739                  |
| Gini                | 0.2554                  |
| Bagging             | 0.1576                  |
| Random Forest       | 0.1413                  |
| AdaBoost            | 0.1576                  |

Based on misclassification error, Logistic Regression performed the best with an error of 0.108, followed closely by K-Nearest Neighbors (KNN) at 0.1359, and Random Forest at 0.1413. These methods show strong predictive performance for the classification task. Ensemble methods like Bagging and AdaBoost had slightly higher errors (~0.1576), while single decision trees (including Entropy, Gini, and Pruned Tree) generally underperformed, with Gini showing the worst result at 0.2554. In terms of computational cost, Logistic Regression is lightweight and interpretable, whereas KNN requires more memory and time for large datasets. Tree-based methods and ensembles are more flexible and better at capturing complex relationships but can be computationally heavier and less interpretable. Overall, Logistic Regression offered the best balance between simplicity, performance, and generalizability in this case.


We are going to see a representation of all the ROC curves together.
```{r, echo=FALSE}
library(pROC)
par(mar = c(5, 5, 4, 8), xpd = TRUE)
tree.prob <- predict(data_tree, newdata = test_data, type = "vector")[,2]  #
roc.tree <- roc(test_data$HeartDisease, tree.prob)

prune.prob <- predict(prune.data, newdata = test_data, type = "vector")[,2]
roc.prune <- roc(test_data$HeartDisease, prune.prob)

bag.prob <- predict(bag_model, newdata = test_data, type = "prob")[,2]
roc.bag <- roc(test_data$HeartDisease, bag.prob)

rf.prob <- predict(rf_model, newdata = test_data, type = "prob")[,2]
roc.rf <- roc(test_data$HeartDisease, rf.prob)

ada.prob <- predict(r.ada, newdata = test_data, type = "prob")[,2]
roc.ada <- roc(test_data$HeartDisease, ada.prob)

glm.prob <- predict(tree.data, test_data, type="vector")[,2]
roc.glm <- roc(test_data$HeartDisease, glm.prob)

plot(roc.tree, col = "blue", lwd = 2, main = "ROC curves - Comparison of methods")

lines(roc.prune, col = "darkgreen", lwd = 2)
lines(roc.bag, col = "orange", lwd = 2)
lines(roc.rf, col = "red", lwd = 2)
lines(roc.ada, col = "purple", lwd = 2)
lines(roc.glm, col = "yellow", lwd = 2)

legend("topright", inset = c(-0.35, 0), legend = c("Simple tree", "Pruning tree", "Bagging", "Random Forest", "AdaBoost", "Logistic"),
       col = c("blue", "darkgreen", "orange", "red", "purple", "yellow"),
       lwd = 2, bty = "n", cex = 0.9)
par(mar = c(5, 4, 4, 2) + 0.1)
cat("AUC Simple Tree :", auc(roc.tree), "\n")
cat("AUC Pruning tree :", auc(roc.prune), "\n")
cat("AUC Bagging :", auc(roc.bag), "\n")
cat("AUC Random Forest :", auc(roc.rf), "\n")
cat("AUC AdaBoost :", auc(roc.ada), "\n")
cat("AUC Logistic :", auc(roc.glm), "\n")
```
The ROC curves show a quite good prediction for all the methods, because for each, the cruve is far from the diagonal curve, which represent randomness.

The logistic regression, which can be considered as the best prdictive method for a heart disease, has for most important parameters Sex, the type of pain in the chest, Cholesterol, Fasting Blood Sugar, Slope of the Peak Exercise ST Segment and Exercise-Induced Angina. These sydroms, all included, can give a chance of knowing whether or not a patient has a heart disease.

For this study the main metric that was used is accuracy. However, in this particular context a false positive doesn’t have the same severity as a false negative.
A false positive is less of a serious issue than a false negative : for instance, since the predictive tool here is a pre-diagnosis tool, predicting a false negative is not really impactful. The patient would then be brought for further analysis and experts could detect that he is healthy. 
However, in the case of a false negative, the patient's disease wouldn’t be noticed which could lead to a worse state. 

Meaning that some errors here are way more important than others, this is why a more complete model comparison should be more based on finding a model with high precision, another approach would be to compare the models using a f_beta metric with beta<1 to favor precision.


# Summary

In conclusion, our machine learning approaches were able globally to successfully predict the state of a patient using its medical data. This work contributes to the field of predictive healthcare analytics and supports the development of better predictive tools by giving methods to investigate further in future studies.
In this study, we found that the best model is the logistic regression model, it gives an accuracy of 89% with high precision (~94%). This model is very interesting because of its high precision meaning that it doesn’t give a lot of false negatives. This is a highly valuable characteristic in the context of predicting a disease as false negative errors are the most unwanted.
Despite high accuracy results, statistical predictive methods should remain decision helping tools and should never be used by themselves in medical context but always with the expertise of a doctor. Still, these models could truly be used to improve the quality of diagnosis or pre-diagnosis by medical experts globally.
